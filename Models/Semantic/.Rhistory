p0 = rlnorm(n)
growthRates<-function(){rnorm(n,mean=1.05,sd = 0.05)}
nextDistrib <- function(p){sample(growthRates()*p,n)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:2){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)))+xlim(c(-10,10))
g+geom_density(aes(x=pops,colour=as.character(times)))+xlim(c(-2,10))
for(t in 1:10){
}
n=10000
p0 = rlnorm(n)
growthRates<-function(){rnorm(n,mean=1.05,sd = 0.05)}
nextDistrib <- function(p){sample(growthRates()*p,n)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:10){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)))+xlim(c(-2,10))
growthRates<-function(){rnorm(n,mean=1,sd = 0.05)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:10){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)))+xlim(c(-2,10))
n=10000
p0 = rlnorm(n)
growthRates<-function(){rnorm(n,mean=1.1,sd = 0.05)}
nextDistrib <- function(p){sample(growthRates()*p,n)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:10){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)))+xlim(c(-2,10))
n=10000
p0 = rlnorm(n)
growthRates<-function(){rnorm(n,mean=1.1,sd = 0.05)}
nextDistrib <- function(p){sample(growthRates()*p,n)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:30){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)))+xlim(c(-2,10))
g=ggplot(data.frame(pops,times),aes(colour=times))
g+geom_density(aes(x=pops))+xlim(c(-2,10))
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)),show.legend=FALSE)+xlim(c(-2,10))
g+geom_density(aes(x=pops,colour=as.character(times)),show.legend=FALSE)+xlim(c(-2,20))
n=100000
p0 = rlnorm(n)
growthRates<-function(){rnorm(n,mean=1.1,sd = 0.05)}
nextDistrib <- function(p){sample(growthRates()*p,n)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:30){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)),show.legend=FALSE)+xlim(c(-2,20))
pops=p0;times=rep(0,n)
p=p0
for(t in 1:50){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)),show.legend=FALSE)+xlim(c(-2,20))
pops=p0;times=rep(0,n)
p=p0
for(t in 1:50){
p=nextDistrib(p)
pops=append(pops,p/mean(p));times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)),show.legend=FALSE)+xlim(c(-2,20))
pops=p0;times=rep(0,n)
p=p0
for(t in 1:50){
p=nextDistrib(p)
pops=append(pops,p-mean(p));times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)),show.legend=FALSE)+xlim(c(-2,20))
p0=runif(n)
growthRates<-function(){rnorm(n,mean=1.1,sd = 0.05)}
nextDistrib <- function(p){sample(growthRates()*p,n)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:50){
p=nextDistrib(p)
pops=append(pops,p-mean(p));times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)),show.legend=FALSE)+xlim(c(-2,20))
pops=p0;times=rep(0,n)
p=p0
for(t in 1:50){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)),show.legend=FALSE)+xlim(c(-2,20))
n=100000
#p0 = rlnorm(n)
p0=runif(n)
growthRates<-function(){rnorm(n,mean=1.1,sd = 0.05)}
nextDistrib <- function(p){sample(growthRates()*p,n)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:100){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
g=ggplot(data.frame(pops,times))
g+geom_density(aes(x=pops,colour=as.character(times)),show.legend=FALSE)+xlim(c(-2,20))
hist(p,plot=FALSE)
p0 = rlnorm(n)
#p0=runif(n)
growthRates<-function(){rnorm(n,mean=1,sd = 0.05)}
nextDistrib <- function(p){sample(growthRates()*p,n)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:100){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
p0=runif(n)
growthRates<-function(){rnorm(n,mean=1,sd = 0.05)}
nextDistrib <- function(p){sample(growthRates()*p,n)}
pops=p0;times=rep(0,n)
p=p0
for(t in 1:100){
p=nextDistrib(p)
pops=append(pops,p);times=append(times,rep(t,n))
show(mean(p))
}
gc()
gc()
spatializedExpMixtureDensity <- function(gridSize,N,alpha,proba=TRUE,rmin=0,rmax=0,Pmax=1,tolThreshold=0,kernel_type="poisson"){
if(rmin==0){rmin = gridSize/N}
if(rmax==0){rmax = gridSize/N}
# patches of the grid are 1 unit size (in r_min/max units)
grid = matrix(0,gridSize,gridSize)
# matrix of coordinates
coords = matrix(c(c(matrix(rep(1:gridSize,gridSize),nrow=gridSize)),c(matrix(rep(1:gridSize,gridSize),nrow=gridSize,byrow=TRUE))),nrow=gridSize^2)
# first draw param distribs ? not needed
# for exp distribs, P_i = 2pi*d_i*r_i^2
#  -> take P from deterministic distrib ; draw r.
for(i in 1:N){
#show(i)
pop_i = Pmax*i^{-alpha}
r_i = runif(1,min=rmin,max=rmax)
d_i = pop_i / (2*pi*(r_i^2))
# find origin of that kernel
#  -> one of points such that : d(bord) > rcut and \forall \vec{x}\in D(rcut),d(\vec{x})<tolThr.
#pot = which(!pseudoClosing(grid>tolThreshold,r_i),arr.ind=TRUE)
#show(length(pot))
#if(length(pot)==0){
#  # Take a point with minimal density ?
#  pot = which(grid==min(grid),arr.ind=TRUE)
#}
# simplify : take deterministiquely (almost, after two exps only two points possible)
# BUT not close to border
#rbord = 2*rmax*log(Pmax/tolThreshold)
rbord = 2*rmax
# random center
if(max(grid)==0){
# random if no center yet
center = matrix(runif(2,min=rbord+1,max=gridSize-rbord),nrow=1)
}
else {
# else find min pop area not too close to border
pot = which(grid==min(grid[(rbord+1):(gridSize-rbord),(rbord+1):(gridSize-rbord)]),arr.ind=TRUE)
row = sample(nrow(pot),1)
center = matrix(pot[row,],nrow=1)
}
# add kernel : use kernlab laplace kernel or other
if(kernel_type=="poisson"){ker=laplacedot(sigma=1/r_i)}
if(kernel_type=="gaussian"){ker=rbfdot(sigma=1/(2*r_i^2))}
#if(kernel_type="quadratic"){ker=} # is quad kernel available ?
grid = grid + (d_i * matrix(kernelMatrix(kernel=laplacedot(sigma=1/r_i),x=coords,y=center),nrow=gridSize))
}
if(proba==TRUE){grid = grid / sum(grid)}
return(grid)
}
test = spatializedExpMixtureDensity(100,2,0.7)
library(kernlab)
test = spatializedExpMixtureDensity(100,10,0.7)
#test = spatializedExpMixtureDensity(100,3,0.7)
test = spatializedExpMixtureDensity(100,3,0.7)
test = spatializedExpMixtureDensity(100,4,0.7)
test = spatializedExpMixtureDensity(100,5,0.7)
spatializedExpMixtureDensity <- function(gridSize,N,alpha,proba=TRUE,rmin=0,rmax=0,Pmax=1,tolThreshold=0,kernel_type="poisson"){
if(rmin==0){rmin = gridSize/N}
if(rmax==0){rmax = gridSize/N}
# patches of the grid are 1 unit size (in r_min/max units)
grid = matrix(0,gridSize,gridSize)
# matrix of coordinates
coords = matrix(c(c(matrix(rep(1:gridSize,gridSize),nrow=gridSize)),c(matrix(rep(1:gridSize,gridSize),nrow=gridSize,byrow=TRUE))),nrow=gridSize^2)
# first draw param distribs ? not needed
# for exp distribs, P_i = 2pi*d_i*r_i^2
#  -> take P from deterministic distrib ; draw r.
for(i in 1:N){
show(i)
pop_i = Pmax*i^{-alpha}
r_i = runif(1,min=rmin,max=rmax)
d_i = pop_i / (2*pi*(r_i^2))
# find origin of that kernel
#  -> one of points such that : d(bord) > rcut and \forall \vec{x}\in D(rcut),d(\vec{x})<tolThr.
#pot = which(!pseudoClosing(grid>tolThreshold,r_i),arr.ind=TRUE)
#show(length(pot))
#if(length(pot)==0){
#  # Take a point with minimal density ?
#  pot = which(grid==min(grid),arr.ind=TRUE)
#}
# simplify : take deterministiquely (almost, after two exps only two points possible)
# BUT not close to border
#rbord = 2*rmax*log(Pmax/tolThreshold)
rbord = 2*rmax
# random center
if(max(grid)==0){
# random if no center yet
center = matrix(runif(2,min=rbord+1,max=gridSize-rbord),nrow=1)
}
else {
# else find min pop area not too close to border
pot = which(grid==min(grid[(rbord+1):(gridSize-rbord),(rbord+1):(gridSize-rbord)]),arr.ind=TRUE)
row = sample(nrow(pot),1)
center = matrix(pot[row,],nrow=1)
}
# add kernel : use kernlab laplace kernel or other
if(kernel_type=="poisson"){ker=laplacedot(sigma=1/r_i)}
if(kernel_type=="gaussian"){ker=rbfdot(sigma=1/(2*r_i^2))}
#if(kernel_type="quadratic"){ker=} # is quad kernel available ?
grid = grid + (d_i * matrix(kernelMatrix(kernel=laplacedot(sigma=1/r_i),x=coords,y=center),nrow=gridSize))
}
if(proba==TRUE){grid = grid / sum(grid)}
return(grid)
}
test = spatializedExpMixtureDensity(100,5,0.7)
test = spatializedExpMixtureDensity(100,4,0.7)
test = spatializedExpMixtureDensity(100,5,0.7)
persp3D(z=test)
library(plot3D)
persp3D(z=test)
test = spatializedExpMixtureDensity(100,10,0.7)
test = spatializedExpMixtureDensity(100,20,0.7)
persp3D(z=test)
library(igraph)
hep(simplify())
help(simplify)
localGraph=list()
localGraph$gg
is.null(localGraph$gg)
48*400
help(try)
#'   ! assuming q > 0
quantilesToHist<-function(q){
mids=c();density=c()
a = 1/length(q)
qq=c(0,q)
for(i in 1:length(q)){mids = append(mids,(qq[i]+qq[i+1])/2);density=append(density,a/(qq[i+1]-qq[i]))}
res=list()
res$mids=mids;res$density=density
return(res)
}
quantilesToHist(1:10)
help("try")
tryCatch({x+1})
tryCatch({x+1},error=function(e){show("tos")})
tryCatch({x+1},error=function(e){return("tos")})
tryCatch({x+1},error=function(e){return(2)})
tryCatch({x+1},error=function(e){res=2})
res
library(raster)
help("focal")
strsplit("1998;2008;2554")
strsplit("1998;2008;2554",";")
strsplit("1998;2008;2554",";")[[1]]
kwNum = 100000
yearRange=c(1976,1977,1978,1979,1980)
year = paste0(as.character(yearRange[1]),"-",as.character(yearRange[length(yearRange)]))
paste0('relevant.relevant_',year,'_full_',kwNum)
paste0('relevant.relevant_',year,'_full_',kwNum)
5*6*3*10*5
5*6*3*10*5*3
5*6*3*10*5*3*20
5*6*3*10*5*7*20
getwd()
pop = load('Data/GibratSim/countriesPop.RData')
pop
CHina
China
library(dplyr)
as.tbl(China)
China[9000:9100,]
as.tbl(China_Historic)
mmax(China$`2010`)
max(China$`2010`)
hist(China$`2010`,breaks=500)
library(ggplot2)
g=ggplot(China)
d=data.frame()
for(j in 5:8){
d=rbind(d,cbind(sort(China[,j],decreasing = TRUE),1:nrow(China),rep(colnames(China)[j],nrow(China))))
}
sort(China[,j],decreasing = TRUE)
1:nrow(China)
d=data.frame()
for(j in 5:8){
d=rbind(d,data.frame(sort(China[,j],decreasing = TRUE),1:nrow(China),rep(colnames(China)[j],nrow(China))))
}
nrow(China)
China[,j]
d=data.frame()
for(j in 5:8){
d=rbind(d,data.frame(sort(China[,j],decreasing = TRUE,na.last = TRUE),1:nrow(China),rep(colnames(China)[j],nrow(China))))
}
colnames(d)<-c("size","rank","year")
colnames(d)
dim(d)
g=ggplot(d)
g+geom_point(aes(x=rnake,y=size,colour=year))
g+geom_point(aes(x=rank,y=size,colour=year))
g+geom_point(aes(x=log(rank),y=log(size),colour=year))
help("geom_bar")
192000 * 109 / 200
192000 * 109 / 200 / 3600
192000 * 109 / 200 / 3600 / 10
192000 * 109 / 200 / 3600 / 20
14/1.38
50*49/2
d=read.csv('/Users/Juste/Documents/ComplexSystems/EnergyPrice/Models/DataCollection/test/data/test_all.csv')
d=read.csv('/Users/Juste/Documents/ComplexSystems/EnergyPrice/Models/DataCollection/test/data/test_all.csv',sep=";")
d
head(d)
d=read.csv('/Users/Juste/Documents/ComplexSystems/EnergyPrice/Models/DataCollection/test/data/test_all.csv',sep=";",header=FALSE)
head(d)
head(d,n = 100)
which(d[,2]==0)
which(d[,2]==2)
unique(d[,2])
help(apply)
help(Matrix)
library(Matrix)
help("Matrix")
library(raster)
help("focal")
NCmisc::estimate.memory()
install.packages("NCmisc")
NCmisc::estimate.memory()
library(NCmisc)
help("estimate.memory")
setwd(paste0(Sys.getenv('CS_HOME'),'/PatentsMining/Models/Semantic'))
library(networkD3)
library(dplyr)
library(igraph)
# TODO : single date overlaps -> use ?
#  http://jokergoo.github.io/circlize/example/grouped_chordDiagram.html
# TODO : inclusion in a shiny app : idem see
#  https://christophergandrud.github.io/networkD3/
wyears = 1980:2012
windowSize=5
kwLimit="100000"
dispth=0.06
ethunit=4.5e-05
# communities as list in time of list of kws
#   list(year1 = list(com1 = c(...), com2 = c(...)))
coms=list()
for(year in wyears){
yearrange=paste0((year-windowSize+1),"-",year);show(year)
currentkws = as.tbl(read.csv(paste0("probas_count_extended/keywords-count-extended_",yearrange,"_kwLimit",kwLimit,'_dispth',dispth,"_ethunit",ethunit,".csv"),sep=";",header=TRUE,stringsAsFactors = FALSE))
#currentkws %>% group_by(V2)
currentcoms = list()
for(i in unlist(unique(currentkws[,2]))){
rows = currentkws[currentkws$community==i,]
# try to name by best techno disp
name = unlist(rows[rows$technodispersion==max(rows$technodispersion),1])[1]
currentcoms[[name]]=unlist(rows[,1])
}
coms[[as.character(year)]]=currentcoms
}
# test independance measures for naming
#pcaname = prcomp(apply(currentkws[,3:11],2,function(col){return((col - min(col))/(max(col)-min(col)))}))
#cor(apply(currentkws[,3:11],2,function(col){return((col - min(col))/(max(col)-min(col)))}))
#summary(pcaname)
# test com size filtering
# for(year in wyears){
#   lengths = sapply(coms[[as.character(year)]],length)
#   #show(sum(lengths)/100)
#   show(length(which(lengths>(sum(lengths)/100)))/length(coms[[as.character(year)]]))
# }
# -> 100 seems ok
similarityIndex <- function(com1,com2){
return(2 * length(intersect(com1,com2))/(length(com1)+length(com2)))
}
#similarityIndex(coms[["1980"]]$`insect trap`,coms[["1981"]]$`insect trap`)
# PB in nameing taking the most disp word : chemicals -> insect trap !
# compute nodes
# -> data.frame with name
# k=1
# for(year in years){
#   for(comname in names(coms[[year]])){
#     nodes[[paste0(comname,"_",year)]]=k
#     k=k+1
#   }
# }
# compute edges
years=as.character(wyears)
#sizeTh=100
sizeQuantile = 0.97
links=list();
nodes=list()
# data.frame with source (id), target (id) and value = weight
novelties=data.frame();cumnovs=c()
k=1;kn=0
for(t in 2:length(years)){
show(years[t])
prec = coms[[years[t-1]]];current = coms[[years[t]]]
cumnov=0
currentsizes=sapply(current,length)
precsizes=sapply(prec,length)
for(i in 1:length(current)){
if(length(current[[i]])>quantile(currentsizes,sizeQuantile)){
if(i%%100==0){show(i/length(current))}
novelty=1
for(j in 1:length(prec)){
if(length(current[[i]])>quantile(precsizes,sizeQuantile)){
weight = similarityIndex(unlist(prec[[j]]),unlist(current[[i]]))
novelty=novelty-weight^2
if(weight>0.01&length(prec[[j]])>20&length(current[[i]]>20)){
# need community names indexing the list
precname=paste0(names(prec)[j],"_",years[t-1]);currentname=paste0(names(current)[i],"_",years[t])
if(!(precname %in% names(nodes))){nodes[[precname]]=kn;kn=kn+1}
if(!(currentname %in% names(nodes))){nodes[[currentname]]=kn;kn=kn+1}
links[[k]] = c(nodes[[precname]],nodes[[currentname]],weight)
k = k + 1
}
}
}
#novelties=rbind(novelties,c(years[t],novelty*length(current[[i]])/sum(unlist(lapply(current,length)))))
#cumnov=cumnov+novelty*length(current[[i]])/sum(unlist(lapply(current,length)))
}
}
#cumnovs=append(cumnovs,cumnov)
}
# plot(years[2:length(years)],cumnovs,type='l')
#
#names(novelties)<-c("year","novelty")
#g=ggplot(novelties,aes(year,novelty))
#g+geom_point()+geom_smooth()
mlinks=as.data.frame(matrix(data = unlist(links),ncol=3,byrow=TRUE))
names(mlinks)<-c("from","to","weight")
#mlinks$weight=1000*mlinks$weight
mnodes = data.frame(id=0:(length(nodes)-1),name=names(nodes))
#plot(graph_from_data_frame(mlinks,vertices=mnodes))
g = graph_from_data_frame(mlinks,vertices=mnodes)
V(g)$year=as.numeric(sapply(V(g)$name,function(x){substring(text=x,first=nchar(x)-3)}))
V(g)$x=2*V(g)$year
V(g)$y[V(g)$year==wyears[1]]=(1:length(which(V(g)$year==wyears[1])))#/length(which(V(g)$year==wyears[1])) # random layout for first row
for(currentyear in wyears[2:length(wyears)]){
V(g)$y[V(g)$year==currentyear]=1:length(which(V(g)$year==currentyear))
currentvertices = V(g)[V(g)$year==currentyear]
for(v in currentvertices){
currentedges = E(g)[V(g)%->%v]
if(length(currentedges)>0){
V(g)$y[V(g)==v] = sum(currentedges$weight/sum(currentedges$weight)*head_of(g,currentedges)$y)
}
}
# put rank for more visibility
#V(g)$y[V(g)$year==currentyear] = rank(V(g)$y[V(g)$year==currentyear],ties.method = "random")/length(which(V(g)$year==currentyear))
}
plot.igraph(g,#layout=layout_as_tree(g),
vertex.size=0.3,vertex.label=NA,#vertex.label.cex=0,
edge.arrow.size=0,edge.width=5*E(g)$weight#,
#edge.curved=TRUE,margin=0
)
V(g)
library(GA)
help(GA)
help(ga)
V(g)$x=V(g)$year
