persp3D(z=test)
library(plot3D)
persp3D(z=test)
test = spatializedExpMixtureDensity(100,10,0.7)
test = spatializedExpMixtureDensity(100,20,0.7)
persp3D(z=test)
library(igraph)
hep(simplify())
help(simplify)
localGraph=list()
localGraph$gg
is.null(localGraph$gg)
48*400
help(try)
#'   ! assuming q > 0
quantilesToHist<-function(q){
mids=c();density=c()
a = 1/length(q)
qq=c(0,q)
for(i in 1:length(q)){mids = append(mids,(qq[i]+qq[i+1])/2);density=append(density,a/(qq[i+1]-qq[i]))}
res=list()
res$mids=mids;res$density=density
return(res)
}
quantilesToHist(1:10)
help("try")
tryCatch({x+1})
tryCatch({x+1},error=function(e){show("tos")})
tryCatch({x+1},error=function(e){return("tos")})
tryCatch({x+1},error=function(e){return(2)})
tryCatch({x+1},error=function(e){res=2})
res
library(raster)
help("focal")
strsplit("1998;2008;2554")
strsplit("1998;2008;2554",";")
strsplit("1998;2008;2554",";")[[1]]
kwNum = 100000
yearRange=c(1976,1977,1978,1979,1980)
year = paste0(as.character(yearRange[1]),"-",as.character(yearRange[length(yearRange)]))
paste0('relevant.relevant_',year,'_full_',kwNum)
paste0('relevant.relevant_',year,'_full_',kwNum)
5*6*3*10*5
5*6*3*10*5*3
5*6*3*10*5*3*20
5*6*3*10*5*7*20
getwd()
pop = load('Data/GibratSim/countriesPop.RData')
pop
CHina
China
library(dplyr)
as.tbl(China)
China[9000:9100,]
as.tbl(China_Historic)
mmax(China$`2010`)
max(China$`2010`)
hist(China$`2010`,breaks=500)
library(ggplot2)
g=ggplot(China)
d=data.frame()
for(j in 5:8){
d=rbind(d,cbind(sort(China[,j],decreasing = TRUE),1:nrow(China),rep(colnames(China)[j],nrow(China))))
}
sort(China[,j],decreasing = TRUE)
1:nrow(China)
d=data.frame()
for(j in 5:8){
d=rbind(d,data.frame(sort(China[,j],decreasing = TRUE),1:nrow(China),rep(colnames(China)[j],nrow(China))))
}
nrow(China)
China[,j]
d=data.frame()
for(j in 5:8){
d=rbind(d,data.frame(sort(China[,j],decreasing = TRUE,na.last = TRUE),1:nrow(China),rep(colnames(China)[j],nrow(China))))
}
colnames(d)<-c("size","rank","year")
colnames(d)
dim(d)
g=ggplot(d)
g+geom_point(aes(x=rnake,y=size,colour=year))
g+geom_point(aes(x=rank,y=size,colour=year))
g+geom_point(aes(x=log(rank),y=log(size),colour=year))
help("geom_bar")
192000 * 109 / 200
192000 * 109 / 200 / 3600
192000 * 109 / 200 / 3600 / 10
192000 * 109 / 200 / 3600 / 20
14/1.38
50*49/2
d=read.csv('/Users/Juste/Documents/ComplexSystems/EnergyPrice/Models/DataCollection/test/data/test_all.csv')
d=read.csv('/Users/Juste/Documents/ComplexSystems/EnergyPrice/Models/DataCollection/test/data/test_all.csv',sep=";")
d
head(d)
d=read.csv('/Users/Juste/Documents/ComplexSystems/EnergyPrice/Models/DataCollection/test/data/test_all.csv',sep=";",header=FALSE)
head(d)
head(d,n = 100)
which(d[,2]==0)
which(d[,2]==2)
unique(d[,2])
help(apply)
help(Matrix)
library(Matrix)
help("Matrix")
library(raster)
help("focal")
NCmisc::estimate.memory()
install.packages("NCmisc")
NCmisc::estimate.memory()
library(NCmisc)
help("estimate.memory")
############
library(Matrix)
library(ggplot2)
library(dplyr)
library(reshape2)
setwd(paste0(Sys.getenv('CS_HOME'),'/PatentsMining/Models/Semantic'))
#wyears = 1980:2012
wyears = 1980:2007
windowSize=5
kwLimit="100000"
dispth=0.06
ethunit="4.1e-05"
classifdir = paste0('classification_window',windowSize,'_kwLimit',kwLimit,'_dispth',dispth,'_ethunit',ethunit)
load(paste0('processed/',classifdir,'/processed_',(year-windowSize+1),"-",year,'.RData'));show(year)
year=1980
load(paste0('processed/',classifdir,'/processed_',(year-windowSize+1),"-",year,'.RData'));show(year)
names(currentprobas)
technoprobas=currentprobas$technoprobas;semprobas=currentprobas$semprobas;primtechnoprobas=currentprobas$technoprobasprim;
dim(primtechnoprobas)
setwd(paste0(Sys.getenv('CS_HOME'),'/PatentsMining/Models/Semantic'))
library(networkD3)
library(dplyr)
library(igraph)
wyears = 1980:2007
windowSize=5
kwLimit="100000"
dispth=0.06
ethunit="4.1e-05"
classifdir = paste0('classification_window',windowSize,'_kwLimit',kwLimit,'_dispth',dispth,'_ethunit',ethunit)
year=1980
wyears = 1980:2007
windowSize=5
kwLimitNum="100000.0"
kwLimit="100000"
dispth=0.06
ethunit="4.1e-05"
classifdir = paste0('classification_window',windowSize,'_kwLimit',kwLimit,'_dispth',dispth,'_ethunit',ethunit)
semprefix = paste0('classification/',classifdir,'/probas_')
semsuffix = paste0('_kwLimit',kwLimitNum,'_dispth',dispth,'_ethunit',ethunit,'.csv')
year
yearrange=paste0((year-windowSize+1),"-",year);show(year)
currentkws = as.tbl(read.csv(file=paste0(semprefix,yearrange,semsuffix),sep=";",header=FALSE,stringsAsFactors = FALSE))
yearrange=paste0((year-windowSize+1),"-",year);show(year)
semprefix = paste0('classification/',classifdir,'/keywords_')
semsuffix = paste0('_kwLimit',kwLimitNum,'_dispth',dispth,'_ethunit',ethunit,'.csv')
# communities as list in time of list of kws
#   list(year1 = list(com1 = c(...), com2 = c(...)))
yearrange=paste0((year-windowSize+1),"-",year);show(year)
currentkws = as.tbl(read.csv(file=paste0(semprefix,yearrange,semsuffix),sep=";",stringsAsFactors = FALSE))
currentkws
unique(currentkws$community)
help("Reduce")
Reduce(paste,c("a","b"))
yearrange=paste0((year-windowSize+1),"-",year);show(year)
currentkws = as.tbl(read.csv(file=paste0(semprefix,yearrange,semsuffix),sep=";",stringsAsFactors = FALSE))
currentcoms = list()
for(i in unlist(unique(currentkws$community))){
rows = currentkws[currentkws$community==i,]
# try to name by best techno disp
name = Reduce(function(s1,s2){return(paste0(s1," ; ",s2))},unlist(rows[rows$technodispersion==max(rows$technodispersion),1])[1:2])
currentcoms[[name]]=unlist(rows[,1])
}
currentcoms
currentcoms[[1]]
names(currentcoms)
yearrange=paste0((year-windowSize+1),"-",year);show(year)
currentkws = as.tbl(read.csv(file=paste0(semprefix,yearrange,semsuffix),sep=";",stringsAsFactors = FALSE))
currentcoms = list()
for(i in unlist(unique(currentkws$community))){
rows = currentkws[currentkws$community==i,]
# try to name by best techno disp
name =unlist(rows[rows$technodispersion==max(rows$technodispersion),1])[1] #Reduce(function(s1,s2){return(paste0(s1," ; ",s2))},unlist(rows[rows$technodispersion==max(rows$technodispersion),1])[1:2])
currentcoms[[name]]=unlist(rows[,1])
}
names(currentcoms)
coms=list()
for(year in wyears){
yearrange=paste0((year-windowSize+1),"-",year);show(year)
currentkws = as.tbl(read.csv(file=paste0(semprefix,yearrange,semsuffix),sep=";",stringsAsFactors = FALSE))
currentcoms = list()
for(i in unlist(unique(currentkws$community))){
rows = currentkws[currentkws$community==i,]
# try to name by best techno disp
name =unlist(rows[rows$technodispersion==max(rows$technodispersion),1])[1] #Reduce(function(s1,s2){return(paste0(s1," ; ",s2))},unlist(rows[rows$technodispersion==max(rows$technodispersion),1])[1:2])
currentcoms[[name]]=unlist(rows[,1])
}
coms[[as.character(year)]]=currentcoms
}
length(coms)
names(coms)
coms[[as.character(year)]]
sapply(length,coms[[as.character(year)]])
help(sapply)
sapply(coms[[as.character(year)]],length)
mean(sapply(coms[[as.character(year)]],length))
meansizes=c();medsizes=c();years=c()
for(year in wyears){
currentcoms = coms[[as.character(year)]]
currentlengths=sapply(currentcoms,length)
meansizes=append(meansizes,mean(currentlengths));medsizes=append(medsizes,quantile(currentlengths,0.5));years=append(years,year)
}
meansizes
medsizes
meansizes=c();medsizes=c();years=c()
for(year in wyears){
currentcoms = coms[[as.character(year)]]
currentlengths=sapply(currentcoms,length)
meansizes=append(meansizes,mean(currentlengths));medsizes=append(medsizes,quantile(currentlengths,0.75));years=append(years,year)
}
medsizes
meansizes=c();medsizes=c();years=c()
for(year in wyears){
currentcoms = coms[[as.character(year)]]
currentlengths=sapply(currentcoms,length)
meansizes=append(meansizes,mean(currentlengths));medsizes=append(medsizes,quantile(currentlengths,0.8));years=append(years,year)
}
medsizes
meansizes=c();medsizes=c();years=c()
for(year in wyears){
currentcoms = coms[[as.character(year)]]
currentlengths=sapply(currentcoms,length)
meansizes=append(meansizes,mean(currentlengths));medsizes=append(medsizes,quantile(currentlengths,0.5));years=append(years,year)
}
ylabel = "community size"
gsum=ggplot(data.frame(meansizes,years),aes(x=years,y=meansizes))
labs=rep("",length(wyears));labs[seq(from=1,to=length(labs),by=5)]=as.character(wyears[seq(from=1,to=length(labs),by=5)])
gsum+geom_point()+geom_line()+
scale_x_discrete(breaks=as.character(wyears),labels=labs)+ylab(paste0("mean ",ylabel))+xlabel("year")+
theme(axis.title = element_text(size = 22), axis.text.x = element_text(size = 15),  axis.text.y = element_text(size = 15))
#ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Analysis/window5/overlap/',measure,'_interclassif_all_ts_semcounts.pdf'),width=10,height=5)
ylabel = "community size"
gsum=ggplot(data.frame(meansizes,years),aes(x=years,y=meansizes))
labs=rep("",length(wyears));labs[seq(from=1,to=length(labs),by=5)]=as.character(wyears[seq(from=1,to=length(labs),by=5)])
gsum+geom_point()+geom_line()+
scale_x_discrete(breaks=as.character(wyears),labels=labs)+ylab(paste0("mean ",ylabel))+xlab("year")+
theme(axis.title = element_text(size = 22), axis.text.x = element_text(size = 15),  axis.text.y = element_text(size = 15))
#ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Analysis/window5/overlap/',measure,'_interclassif_all_ts_semcounts.pdf'),width=10,height=5)
ylabel = "community size"
gsum=ggplot(data.frame(meansizes,years),aes(x=years,y=meansizes))
labs=rep("",length(wyears));labs[seq(from=1,to=length(labs),by=5)]=as.character(wyears[seq(from=1,to=length(labs),by=5)])
gsum+geom_point()+geom_line()+
#scale_x_discrete(breaks=as.character(wyears),labels=labs)+
ylab(paste0("mean ",ylabel))+xlab("year")+
theme(axis.title = element_text(size = 22), axis.text.x = element_text(size = 15),  axis.text.y = element_text(size = 15))
#ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Analysis/window5/overlap/',measure,'_interclassif_all_ts_semcounts.pdf'),width=10,height=5)
ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Analysis/window5/sizes/meancomsize.pdf'),width=10,height=5)
load('res/modularities.RData')
df = data.frame(year=sapply(modularities,function(l){l$year}),
technoovmod=sapply(modularities,function(l){l$technoovmod}),
semovmod=sapply(modularities,function(l){l$semovmod}),
technodirmod=sapply(modularities,function(l){l$technodirmod}),
semdirmod=sapply(modularities,function(l){l$semdirmod}),
technodirgraphmod=sapply(modularities,function(l){l$technodirgraphmod}),
semdirgraphmod=sapply(modularities,function(l){l$semdirgraphmod}),
technoundirgraphmod=sapply(modularities,function(l){l$technoundirgraphmod}),
semundirgraphmod=sapply(modularities,function(l){l$semundirgraphmod})
)
df
g=ggplot(data.frame(year=c(df$year,df$year),mod = c(df$technoovmod,df$semovmod),type=c(rep("technological",nrow(df)),rep("semantic",nrow(df)))),aes(x=year,y=mod,colour=type,group=type))
g+geom_line()+geom_point()+ylab("overlapping modularity")+scale_y_log10()+
theme(legend.position = "none",axis.title = element_text(size = 22), axis.text.x = element_text(size = 15),  axis.text.y = element_text(size = 15))
ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Analysis/window5/citation/overlappingmodularity.pdf'),width=10,height=5)
g=ggplot(data.frame(year=c(df$year,df$year),mod = c(df$technoovmod,df$semovmod),type=c(rep("technological",nrow(df)),rep("semantic",nrow(df)))),aes(x=year,y=mod,colour=type,group=type))
g+geom_line()+geom_point()+ylab("overlapping modularity")+#scale_y_log10()+
theme(legend.position = "none",axis.title = element_text(size = 22), axis.text.x = element_text(size = 15),  axis.text.y = element_text(size = 15))
ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Analysis/window5/citation/overlappingmodularity.pdf'),width=10,height=5)
g=ggplot(data.frame(year=c(df$year,df$year),mod = c(df$technoovmod,df$semovmod),type=c(rep("technological",nrow(df)),rep("semantic",nrow(df)))),aes(x=year,y=mod,colour=type,group=type))
g+geom_line()+geom_point()+ylab("overlapping modularity")+#scale_y_log10()+
theme(axis.title = element_text(size = 22), axis.text.x = element_text(size = 15),  axis.text.y = element_text(size = 15))
ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Analysis/window5/citation/overlappingmodularity.pdf'),width=10,height=5)
g=ggplot(data.frame(year=c(df$year,df$year),mod = c(df$technodirmod,df$semdirmod),type=c(rep("technological",nrow(df)),rep("semantic",nrow(df)))),aes(x=year,y=mod,colour=type,group=type))
g+geom_line()+geom_point()+ylab("modularity")+#scale_y_log10()+
theme(axis.title = element_text(size = 22), axis.text.x = element_text(size = 15),  axis.text.y = element_text(size = 15))
ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Analysis/window5/citation/simplemodularity.pdf'),width=10,height=5)
df$technoovmod[df$year==2007]
df$semovmod[df$year==2007]
similarityIndex <- function(com1,com2){
return(2 * length(intersect(com1,com2))/(length(com1)+length(com2)))
}
years=as.character(wyears)
#sizeTh=100
sizeQuantile = 0.97
links=list();
nodes=list()
# data.frame with source (id), target (id) and value = weight
novelties=data.frame();cumnovs=c()
k=1;kn=0
for(t in 2:length(years)){
show(years[t])
prec = coms[[years[t-1]]];current = coms[[years[t]]]
cumnov=0
currentsizes=sapply(current,length)
precsizes=sapply(prec,length)
for(i in 1:length(current)){
if(length(current[[i]])>quantile(currentsizes,sizeQuantile)){
if(i%%100==0){show(i/length(current))}
novelty=1
for(j in 1:length(prec)){
if(length(current[[i]])>quantile(precsizes,sizeQuantile)){
weight = similarityIndex(unlist(prec[[j]]),unlist(current[[i]]))
novelty=novelty-weight^2
if(weight>0.01&length(prec[[j]])>20&length(current[[i]]>20)){
# need community names indexing the list
precname=paste0(names(prec)[j],"_",years[t-1]);currentname=paste0(names(current)[i],"_",years[t])
if(!(precname %in% names(nodes))){nodes[[precname]]=kn;kn=kn+1}
if(!(currentname %in% names(nodes))){nodes[[currentname]]=kn;kn=kn+1}
links[[k]] = c(nodes[[precname]],nodes[[currentname]],weight)
k = k + 1
}
}
}
#novelties=rbind(novelties,c(years[t],novelty*length(current[[i]])/sum(unlist(lapply(current,length)))))
#cumnov=cumnov+novelty*length(current[[i]])/sum(unlist(lapply(current,length)))
}
}
#cumnovs=append(cumnovs,cumnov)
}
# plot(years[2:length(years)],cumnovs,type='l')
#
#names(novelties)<-c("year","novelty")
#g=ggplot(novelties,aes(year,novelty))
#g+geom_point()+geom_smooth()
mlinks=as.data.frame(matrix(data = unlist(links),ncol=3,byrow=TRUE))
names(mlinks)<-c("from","to","weight")
#mlinks$weight=1000*mlinks$weight
mnodes = data.frame(id=0:(length(nodes)-1),name=names(nodes))
#plot(graph_from_data_frame(mlinks,vertices=mnodes))
g = graph_from_data_frame(mlinks,vertices=mnodes)
V(g)$year=as.numeric(sapply(V(g)$name,function(x){substring(text=x,first=nchar(x)-3)}))
g
V(g)$comname = sapply(V(g)$name,function(x){substring(text=x,last=nchar(x)-3)})
V(g)$comname = sapply(V(g)$name,function(x){substring(text=x,first=1,last=nchar(x)-3)})
head(V(g)$comname)
V(g)$comname = sapply(V(g)$name,function(x){substring(text=x,first=1,last=nchar(x)-5)})
head(V(g)$comname)
# specific algo for layout, using weight proximity
V(g)$x=V(g)$year
V(g)$y[V(g)$year==wyears[1]]=(1:length(which(V(g)$year==wyears[1])))#/length(which(V(g)$year==wyears[1])) # random layout for first row
for(currentyear in wyears[2:length(wyears)]){
V(g)$y[V(g)$year==currentyear]=1:length(which(V(g)$year==currentyear))
currentvertices = V(g)[V(g)$year==currentyear]
for(v in currentvertices){
currentedges = E(g)[V(g)%->%v]
if(length(currentedges)>0){
V(g)$y[V(g)==v] = sum(currentedges$weight/sum(currentedges$weight)*head_of(g,currentedges)$y)
}
}
# put rank for more visibility
#V(g)$y[V(g)$year==currentyear] = rank(V(g)$y[V(g)$year==currentyear],ties.method = "random")/length(which(V(g)$year==currentyear))
}
plot.igraph(g,#layout=layout_as_tree(g),
vertex.size=0.3,vertex.label=NA,#vertex.label.cex=0,
edge.arrow.size=0,edge.width=5*E(g)$weight#,
#edge.curved=TRUE,margin=0
)
V(g)$x=V(g)$year
V(g)$y[V(g)$year==wyears[1]]=(1:length(which(V(g)$year==wyears[1])))#/length(which(V(g)$year==wyears[1])) # random layout for first row
for(currentyear in wyears[2:length(wyears)]){
V(g)$y[V(g)$year==currentyear]=1:length(which(V(g)$year==currentyear))
currentvertices = V(g)[V(g)$year==currentyear]
for(v in currentvertices){
currentedges = E(g)[V(g)%->%v]
if(length(currentedges)>0){
V(g)$y[V(g)==v] = sum(currentedges$weight/sum(currentedges$weight)*head_of(g,currentedges)$y)
}
}
# put rank for more visibility
#V(g)$y[V(g)$year==currentyear] = rank(V(g)$y[V(g)$year==currentyear],ties.method = "random")/length(which(V(g)$year==currentyear))
V(g)$y[V(g)$year==currentyear] = (V(g)$y[V(g)$year==currentyear] - min(V(g)$y[V(g)$year==currentyear]))/(max(V(g)$y[V(g)$year==currentyear])-min(V(g)$y[V(g)$year==currentyear]))
}
plot.igraph(g,#layout=layout_as_tree(g),
vertex.size=0.3,vertex.label=NA,#vertex.label.cex=0,
edge.arrow.size=0,edge.width=5*E(g)$weight#,
#edge.curved=TRUE,margin=0
)
V(g)$y[V(g)$year==wyears[1]]=(1:length(which(V(g)$year==wyears[1])))/length(which(V(g)$year==wyears[1])) # random layout for first row
for(currentyear in wyears[2:length(wyears)]){
V(g)$y[V(g)$year==currentyear]=1:length(which(V(g)$year==currentyear))
currentvertices = V(g)[V(g)$year==currentyear]
for(v in currentvertices){
currentedges = E(g)[V(g)%->%v]
if(length(currentedges)>0){
V(g)$y[V(g)==v] = sum(currentedges$weight/sum(currentedges$weight)*head_of(g,currentedges)$y)
}
}
# put rank for more visibility
#V(g)$y[V(g)$year==currentyear] = rank(V(g)$y[V(g)$year==currentyear],ties.method = "random")/length(which(V(g)$year==currentyear))
V(g)$y[V(g)$year==currentyear] = (V(g)$y[V(g)$year==currentyear] - min(V(g)$y[V(g)$year==currentyear]))/(max(V(g)$y[V(g)$year==currentyear])-min(V(g)$y[V(g)$year==currentyear]))
}
plot.igraph(g,#layout=layout_as_tree(g),
vertex.size=0.3,vertex.label=NA,#vertex.label.cex=0,
edge.arrow.size=0,edge.width=5*E(g)$weight#,
#edge.curved=TRUE,margin=0
)
max(V(g)$year==wyears[1]))
max(V(g)$year==wyears[1])
max(V(g)$y[V(g)$year==wyears[1]])
V(g)$y = runif(vcount(g))
plot.igraph(g,#layout=layout_as_tree(g),
vertex.size=0.3,vertex.label=NA,#vertex.label.cex=0,
edge.arrow.size=0,edge.width=5*E(g)$weight#,
#edge.curved=TRUE,margin=0
)
V(g)$y = runif(vcount(g))
plot.igraph(g,#layout=layout_as_tree(g),
vertex.size=0.3,vertex.label=NA,#vertex.label.cex=0,
edge.arrow.size=0,edge.width=5*E(g)$weight#,
#edge.curved=TRUE,margin=0
)
help("%->%")
incoming = E(g)[which(V(g)$year==(currentyear-1))%->%which(V(g)$year==currentyear)]
currentyear=wyears[1]
currentyear=wyears[2]
incoming = E(g)[which(V(g)$year==(currentyear-1))%->%which(V(g)$year==currentyear)]
incoming
diff = abs(head_of(incoming)$y - tail_of(incoming)$y)*incoming$weight
head_of(incoming)$y
head_of(incoming)
incoming
help(head_of)
diff = abs(head_of(g,incoming)$y - tail_of(g,incoming)$y)*incoming$weight
max(diff)
hist(diff,breaks=20)
summary(incoming$weight)
library(ggplot2)
library(dplyr)
setwd(paste0(Sys.getenv('CS_HOME'),'/PatentsMining/Models/Semantic'))
# analysis of network modularities/size to parameters
#wyears = 1980:2007
wyears = 1978:2007
windowSize=3
kwLimit="100000.0"
eth="10.0"
install.packages("anet")
install.packages("nea")
wyears = 1978:2007
library(ggplot2)
library(dplyr)
setwd(paste0(Sys.getenv('CS_HOME'),'/PatentsMining/Models/Semantic'))
# analysis of network modularities/size to parameters
#wyears = 1980:2007
wyears = 1978:2007
windowSize=3;windowStr="3"
kwLimit="100000.0"
eth="10.0"
yearlycount=read.csv(file=paste0('data/patentcount_window',windowStr,'.csv'),sep=";",header=TRUE,stringsAsFactors = FALSE)
yearlycount
plot(wyears,yearlycount$count)
length(wyears)
nrow(yearlycount)
plot(wyears,yearlycount$count[1:30])
plot(wyears,yearlycount$count[1:30],type='l')
ethunit=4.1e-5
# -> mean argmax on all dispth ; lay on pareto front (comnum,vcount) (or very near) for each year
argmax=c()
alldata=data.frame()
projval=c();projtype=c();projdisp=c();projyear=c()
for(year in wyears){
#year=1980
show(year)
yearrange = paste0((year-windowSize+1),"-",year)
npatents=yearlycount[yearlycount$yearrange==yearrange,2]
sensdata = as.tbl(read.csv(file=paste0('sensitivity/sensitivity_',yearrange,"_",kwLimit,"_eth",eth,".csv"),sep=";",header=TRUE))
#sensdata=sensdata[sensdata$eth<60,]
sensdata = sensdata %>% group_by(dispth,eth) %>% summarise(modularity=max(modularity),comnum=min(comnum),vcount=mean(vcount))
argmaxs=sensdata%>% group_by(dispth)%>%summarise(argmaxeth=eth[comnum==max(comnum)][1])
argmax=append(argmax,mean(argmaxs$argmaxeth)/npatents)
ethunitdist = log(1+abs((ethunit*npatents) - sensdata$eth))
alldata=rbind(alldata,cbind(sensdata,
year=rep(as.character(year),nrow(sensdata)),
dist=abs(sensdata$eth-mean(argmaxs$argmaxeth)),
ethunitdist=ethunitdist
)
)
projrows = which(ethunitdist==min(ethunitdist))
projval=append(projval,c(sensdata$comnum[projrows]/max(sensdata$comnum[projrows]),sensdata$vcount[projrows]/max(sensdata$vcount[projrows]),sensdata$modularity[projrows]/max(sensdata$modularity[projrows])))
projtype=append(projtype,c(rep("number of communities",length(projrows)),rep("number of vertices",length(projrows)),rep("modularity",length(projrows))))
projdisp=append(projdisp,rep(sensdata$dispth[projrows],3))
projyear=append(projyear,rep(as.character(year),3*length(projrows)))
}
g=ggplot(data.frame(dispth=projdisp,val=projval,objective=projtype,year=projyear))
g+geom_line(aes(x=dispth,y=val,col=objective,group=objective))+facet_wrap(~year)+
xlab(expression(theta[c]))+ylab("normalized objectives")#+ theme(axis.title = element_text(size = 22), axis.text.x = element_text(size = 15),   axis.text.y = element_text(size = 15))
currentdata=alldata[alldata$year==year,] %>% group_by(dispth,eth) %>% summarise(maxmod=max(modularity),comnum=min(comnum),vcount=mean(vcount))
g=ggplot(currentdata)
g+geom_point(aes(x=comnum,y=vcount,colour=maxmod),size=1)+
geom_point(mapping=aes(x=comnum,y=vcount),data = currentdata[currentdata$eth==40,],colour='purple',size=2,pch=1)+
geom_point(mapping=aes(x=comnum,y=vcount),data = currentdata[currentdata$eth==40&currentdata$dispth==0.06,],colour='red',size=4,pch=22)+
xlab("number of communities")+ylab("number of vertices")+scale_colour_continuous(name="modularity")+ theme(axis.title = element_text(size = 22), axis.text.x = element_text(size = 15),   axis.text.y = element_text(size = 15))
ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Sensitivity/window',windowStr,'/comnum_vcount_pareto_',year,'.pdf'),width=10,height=7)
g=ggplot(alldata %>% group_by(dispth,eth,year) %>% summarise(maxmod=max(modularity),comnum=min(comnum),vcount=mean(vcount)))
g+geom_line(aes(x=eth,y=comnum,colour=dispth,group=dispth))+facet_wrap(~year)+
xlab(expression(theta[w]))+ylab("number of communities")+scale_colour_continuous(name=expression(theta[c]))
g+geom_point(aes(x=comnum,y=vcount,colour=maxmod),size=1)+facet_wrap(~year)+
xlab("number of communities")+ylab("number of vertices")+scale_colour_continuous(name="modularity")
g+geom_point(aes(x=comnum,y=vcount,colour=maxmod),size=1)+facet_wrap(~year)+
xlab("number of communities")+ylab("number of vertices")+scale_colour_continuous(name="modularity")
ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Sensitivity/window',windowStr,'/vcount_comnum_pareto_window',windowStr,'.pdf'),width=30,height=20)
g+geom_point(aes(x=comnum,y=vcount,colour=maxmod),size=1)+facet_wrap(~year)+
xlab("number of communities")+ylab("number of vertices")+scale_colour_continuous(name="modularity")
ggsave(file=paste0(Sys.getenv("CS_HOME"),'/PatentsMining/Results/Semantic/Sensitivity/window',windowStr,'/vcount_comnum_pareto_window',windowStr,'.pdf'),width=10,height=7)
